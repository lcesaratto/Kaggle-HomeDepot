{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN with 3 categories, 2 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "df_train_complete_modified = pd.read_csv('train_complete_v4.csv')\n",
    "df_train_complete_modified.set_index('product_uid', inplace=True)\n",
    "df_train_complete_modified = df_train_complete_modified.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74067/74067 [02:25<00:00, 508.04it/s]\n",
      "100%|██████████| 74067/74067 [00:20<00:00, 3572.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74067, 10, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74067/74067 [02:57<00:00, 416.17it/s]\n",
      "100%|██████████| 74067/74067 [00:52<00:00, 1406.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74067, 29, 512)\n",
      "74067\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "words_search_term = [t.split(' ') for t in df_train_complete_modified['search_term']]\n",
    "max_length_search_term = max(len(w) for w in words_search_term)\n",
    "embeddings_search_term = [embed(w).numpy() for w in tqdm(words_search_term)]\n",
    "missing_search_term = [max_length_search_term - e.shape[0] for e in embeddings_search_term]\n",
    "\n",
    "padding = [0] * 512\n",
    "for i, m in enumerate(tqdm(missing_search_term)):\n",
    "    if m == 0:\n",
    "        continue\n",
    "    embeddings_search_term[i] = np.concatenate([embeddings_search_term[i], np.array([padding] * m)])\n",
    "\n",
    "embeddings_search_term = np.array(embeddings_search_term)\n",
    "print(embeddings_search_term.shape) #(74067, 10, 512)\n",
    "#------------------------------------------------------------------------------------------------------------------------------\n",
    "words_product_title = [t.split(' ') for t in df_train_complete_modified['product_title']]\n",
    "max_length_product_title = max(len(w) for w in words_product_title)\n",
    "embeddings_product_title = [embed(w).numpy() for w in tqdm(words_product_title)]\n",
    "missing_product_title = [max_length_product_title - e.shape[0] for e in embeddings_product_title]\n",
    "\n",
    "padding = [0] * 512\n",
    "for i, m in enumerate(tqdm(missing_product_title)):\n",
    "    if m == 0:\n",
    "        continue\n",
    "    embeddings_product_title[i] = np.concatenate([embeddings_product_title[i], np.array([padding] * m)])\n",
    "\n",
    "embeddings_product_title = np.array(embeddings_product_title)\n",
    "print(embeddings_product_title.shape) #(74067, 29, 512)\n",
    "#------------------------------------------------------------------------------------------------------------------------------\n",
    "#y = df_train_complete_modified['relevance'].round().astype(int).tolist()\n",
    "y = df_train_complete_modified['relevance'].tolist()\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74067\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_search_term, X_test_search_term, y_train_search_term, y_test_search_term = train_test_split(embeddings_search_term,\n",
    "                                                                                                   y,\n",
    "                                                                                                   test_size=0.1,\n",
    "                                                                                                   random_state=42,\n",
    "                                                                                                   #stratify=y\n",
    "                                                                                                   )\n",
    "\n",
    "X_train_search_term, X_val_search_term, y_train_search_term, y_val_search_term = train_test_split(X_train_search_term,\n",
    "                                                                                                    y_train_search_term,\n",
    "                                                                                                    test_size=0.2,\n",
    "                                                                                                    random_state=42,\n",
    "                                                                                                    #stratify=y_train_search_term\n",
    "                                                                                                    )\n",
    "\n",
    "\n",
    "X_train_product_title, X_test_product_title, y_train_product_title, y_test_product_title = train_test_split(embeddings_product_title,\n",
    "                                                                                                           y,\n",
    "                                                                                                           test_size=0.1,\n",
    "                                                                                                           random_state=42,\n",
    "                                                                                                           #stratify=y\n",
    "                                                                                                           )\n",
    "\n",
    "X_train_product_title, X_val_product_title, y_train_product_title, y_val_product_title = train_test_split(X_train_product_title,\n",
    "                                                                                                           y_train_product_title,\n",
    "                                                                                                           test_size=0.2,\n",
    "                                                                                                           random_state=42,\n",
    "                                                                                                           #stratify=y_train_product_title\n",
    "                                                                                                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 53328 samples, validate on 13332 samples\n",
      "Epoch 1/100\n",
      "53328/53328 [==============================] - 13s 247us/step - loss: 0.7031 - mse: 0.7031 - val_loss: 0.2768 - val_mse: 0.2768\n",
      "\n",
      "Epoch 00001: val_mse improved from inf to 0.27680, saving model to ./model_data/weights.001-0.2768.hdf5\n",
      "Epoch 2/100\n",
      "53328/53328 [==============================] - 13s 243us/step - loss: 0.3644 - mse: 0.3644 - val_loss: 0.2702 - val_mse: 0.2702\n",
      "\n",
      "Epoch 00002: val_mse improved from 0.27680 to 0.27022, saving model to ./model_data/weights.002-0.2702.hdf5\n",
      "Epoch 3/100\n",
      "53328/53328 [==============================] - 13s 244us/step - loss: 0.3414 - mse: 0.3414 - val_loss: 0.2661 - val_mse: 0.2661\n",
      "\n",
      "Epoch 00003: val_mse improved from 0.27022 to 0.26610, saving model to ./model_data/weights.003-0.2661.hdf5\n",
      "Epoch 4/100\n",
      "53328/53328 [==============================] - 13s 242us/step - loss: 0.3291 - mse: 0.3291 - val_loss: 0.2668 - val_mse: 0.2668\n",
      "\n",
      "Epoch 00004: val_mse did not improve from 0.26610\n",
      "Epoch 5/100\n",
      "53328/53328 [==============================] - 13s 242us/step - loss: 0.3123 - mse: 0.3123 - val_loss: 0.2570 - val_mse: 0.2570\n",
      "\n",
      "Epoch 00005: val_mse improved from 0.26610 to 0.25699, saving model to ./model_data/weights.005-0.2570.hdf5\n",
      "Epoch 6/100\n",
      "53328/53328 [==============================] - 13s 241us/step - loss: 0.2989 - mse: 0.2989 - val_loss: 0.2543 - val_mse: 0.2543\n",
      "\n",
      "Epoch 00006: val_mse improved from 0.25699 to 0.25429, saving model to ./model_data/weights.006-0.2543.hdf5\n",
      "Epoch 7/100\n",
      "53328/53328 [==============================] - 13s 243us/step - loss: 0.2899 - mse: 0.2899 - val_loss: 0.2655 - val_mse: 0.2655\n",
      "\n",
      "Epoch 00007: val_mse did not improve from 0.25429\n",
      "Epoch 8/100\n",
      "53328/53328 [==============================] - 13s 241us/step - loss: 0.2760 - mse: 0.2760 - val_loss: 0.2608 - val_mse: 0.2608\n",
      "\n",
      "Epoch 00008: val_mse did not improve from 0.25429\n",
      "Epoch 9/100\n",
      "53328/53328 [==============================] - 13s 241us/step - loss: 0.2672 - mse: 0.2672 - val_loss: 0.2509 - val_mse: 0.2509\n",
      "\n",
      "Epoch 00009: val_mse improved from 0.25429 to 0.25090, saving model to ./model_data/weights.009-0.2509.hdf5\n",
      "Epoch 10/100\n",
      "53328/53328 [==============================] - 13s 241us/step - loss: 0.2595 - mse: 0.2595 - val_loss: 0.2512 - val_mse: 0.2512\n",
      "\n",
      "Epoch 00010: val_mse did not improve from 0.25090\n",
      "Epoch 11/100\n",
      "53328/53328 [==============================] - 13s 239us/step - loss: 0.2466 - mse: 0.2466 - val_loss: 0.2580 - val_mse: 0.2580\n",
      "\n",
      "Epoch 00011: val_mse did not improve from 0.25090\n",
      "Epoch 12/100\n",
      "53328/53328 [==============================] - 13s 238us/step - loss: 0.2376 - mse: 0.2376 - val_loss: 0.2482 - val_mse: 0.2482\n",
      "\n",
      "Epoch 00012: val_mse improved from 0.25090 to 0.24824, saving model to ./model_data/weights.012-0.2482.hdf5\n",
      "Epoch 13/100\n",
      "53328/53328 [==============================] - 13s 239us/step - loss: 0.2309 - mse: 0.2309 - val_loss: 0.2657 - val_mse: 0.2657\n",
      "\n",
      "Epoch 00013: val_mse did not improve from 0.24824\n",
      "Epoch 14/100\n",
      "53328/53328 [==============================] - 13s 243us/step - loss: 0.2259 - mse: 0.2259 - val_loss: 0.2510 - val_mse: 0.2510\n",
      "\n",
      "Epoch 00014: val_mse did not improve from 0.24824\n",
      "Epoch 15/100\n",
      "53328/53328 [==============================] - 13s 241us/step - loss: 0.2163 - mse: 0.2163 - val_loss: 0.2550 - val_mse: 0.2550\n",
      "\n",
      "Epoch 00015: val_mse did not improve from 0.24824\n",
      "Epoch 16/100\n",
      "53328/53328 [==============================] - 13s 241us/step - loss: 0.2076 - mse: 0.2076 - val_loss: 0.2603 - val_mse: 0.2603\n",
      "\n",
      "Epoch 00016: val_mse did not improve from 0.24824\n",
      "Epoch 17/100\n",
      "53328/53328 [==============================] - 13s 243us/step - loss: 0.2032 - mse: 0.2032 - val_loss: 0.2545 - val_mse: 0.2545\n",
      "\n",
      "Epoch 00017: val_mse did not improve from 0.24824\n",
      "Epoch 18/100\n",
      "53328/53328 [==============================] - 13s 240us/step - loss: 0.1959 - mse: 0.1959 - val_loss: 0.2558 - val_mse: 0.2558\n",
      "\n",
      "Epoch 00018: val_mse did not improve from 0.24824\n",
      "Epoch 19/100\n",
      "53328/53328 [==============================] - 13s 243us/step - loss: 0.1913 - mse: 0.1913 - val_loss: 0.2552 - val_mse: 0.2552\n",
      "\n",
      "Epoch 00019: val_mse did not improve from 0.24824\n",
      "Epoch 20/100\n",
      "53328/53328 [==============================] - 13s 241us/step - loss: 0.1848 - mse: 0.1848 - val_loss: 0.2620 - val_mse: 0.2620\n",
      "\n",
      "Epoch 00020: val_mse did not improve from 0.24824\n",
      "Epoch 21/100\n",
      "53328/53328 [==============================] - 13s 242us/step - loss: 0.1803 - mse: 0.1803 - val_loss: 0.2588 - val_mse: 0.2588\n",
      "\n",
      "Epoch 00021: val_mse did not improve from 0.24824\n",
      "Epoch 22/100\n",
      "53328/53328 [==============================] - 13s 243us/step - loss: 0.1739 - mse: 0.1739 - val_loss: 0.2680 - val_mse: 0.2680\n",
      "\n",
      "Epoch 00022: val_mse did not improve from 0.24824\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------------------------------------------------------------------------------------------------------------\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, merge, MaxPooling1D, Dropout, Conv1D, concatenate, Reshape, Flatten, Dropout\n",
    "from keras.utils import plot_model\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 256\n",
    "# Search term CNN\n",
    "input_search_term = Input(shape=(max_length_search_term, 512))\n",
    "conv1d_search_term = Conv1D(filters=32, kernel_size=7, activation='relu')(input_search_term)\n",
    "maxpooling1d_search_term = MaxPooling1D(pool_size=2, strides=2)(conv1d_search_term)\n",
    "conv1d_search_term_2 = Conv1D(filters=64, kernel_size=3, activation='relu', padding='same')(maxpooling1d_search_term)\n",
    "maxpooling1d_search_term_2 = MaxPooling1D(pool_size=2, strides=2)(conv1d_search_term_2)\n",
    "dropout_search_term = (Dropout(0.5))(maxpooling1d_search_term_2)\n",
    "flatten_search_term = Flatten()(dropout_search_term)\n",
    "\n",
    "# Product title CNN\n",
    "input_product_title = Input(shape=(max_length_product_title, 512))\n",
    "conv1d_product_title = Conv1D(filters=32, kernel_size=7, activation='relu')(input_product_title)\n",
    "maxpooling1d_product_title = MaxPooling1D(pool_size=2, strides=2)(conv1d_product_title)\n",
    "conv1d_product_title_2 = Conv1D(filters=64, kernel_size=3, activation='relu', padding='same')(maxpooling1d_product_title)\n",
    "maxpooling1d_product_title_2 = MaxPooling1D(pool_size=2, strides=2)(conv1d_product_title_2)\n",
    "dropout_product_title = (Dropout(0.5))(maxpooling1d_product_title_2)\n",
    "flatten_product_title = Flatten()(dropout_product_title)\n",
    "\n",
    "# concatenated model\n",
    "concatenated_layers = concatenate([flatten_search_term, flatten_product_title])\n",
    "model_concatenated = Dense(80, activation=\"relu\")(concatenated_layers)\n",
    "dropout = (Dropout(0.5))(concatenated_layers)\n",
    "model_output = Dense(1, activation=\"linear\")(dropout)\n",
    "\n",
    "model = Model(inputs= [input_search_term, input_product_title], outputs=model_output)\n",
    "\n",
    "checkpoint = ModelCheckpoint('./model_data/' + 'weights.{epoch:03d}-{val_mse:.4f}.hdf5',\n",
    "                                 monitor='val_mse', verbose=1,\n",
    "                                 save_best_only=True, mode='auto')\n",
    "\n",
    "cb = EarlyStopping(monitor='val_mse',\n",
    "                              min_delta=0,\n",
    "                              patience=10,\n",
    "                              verbose=1,\n",
    "                              mode='auto')\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n",
    "\n",
    "model.fit([X_train_search_term, X_train_product_title], y_train_search_term, epochs=epochs, batch_size=batch_size ,verbose=1, callbacks=[checkpoint, cb],\n",
    "          validation_data=([X_val_search_term, X_val_product_title], y_val_search_term), class_weight=\"auto\")\n",
    "\n",
    "model.save_weights('./model_data/' + 'final weights')\n",
    "model.save('./model_data/' + 'my_model.h5')\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.2485610057742038\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "import glob\n",
    "\n",
    "# load model\n",
    "#model = load_model('./model_data/' + \"my_model.h5\")\n",
    "#model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n",
    "# find the file with the best weights\n",
    "list_of_files = glob.glob('./model_data/' + 'weights.*.hdf5')\n",
    "#youngest_file = get_youngest_file(list_of_files)\n",
    "# load weights into model\n",
    "model.load_weights('model_data/weights.012-0.2482.hdf5')\n",
    "\n",
    "prediction = model.predict([X_test_search_term, X_test_product_title])\n",
    "prediction = prediction.tolist()\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------\n",
    "from sklearn.metrics import accuracy_score, precision_score, classification_report, confusion_matrix, mean_squared_error\n",
    "\n",
    "print (\"MSE:\", mean_squared_error(y_test_search_term, prediction))\n",
    "# print (\"Precision:\", precision_score(y_test, prediction, average ='micro'))\n",
    "#print (classification_report(y_test_search_term, prediction))\n",
    "# print (confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.7402754686486568\n"
     ]
    }
   ],
   "source": [
    "prediction = [y[0] for y in prediction]\n",
    "df_result = pd.DataFrame({'Relevance': y_test_search_term, 'Prediction':prediction})\n",
    "mask = df_result.Relevance < 2\n",
    "print (\"MSE:\", mean_squared_error(df_result[mask]['Relevance'], df_result[mask]['Prediction']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class TextSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, field):\n",
    "        self.field = field\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.field]\n",
    "    \n",
    "class NumberSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, field):\n",
    "        self.field = field\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[[self.field]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "def Tokenizer(str_input):\n",
    "    words = re.sub(r\"[^A-Za-z0-9\\-]\", \" \", str_input).lower().split()\n",
    "    porter_stemmer=nltk.PorterStemmer()\n",
    "    words = [porter_stemmer.stem(word) for word in words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "classifier = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('text', Pipeline([\n",
    "            ('colext', TextSelector('Text')),\n",
    "            ('tfidf', TfidfVectorizer(tokenizer=Tokenizer, stop_words=stop_words,\n",
    "                     min_df=.0025, max_df=0.25, ngram_range=(1,3))),\n",
    "            ('svd', TruncatedSVD(algorithm='randomized', n_components=300)), #for XGB\n",
    "        ])),\n",
    "        ('words', Pipeline([\n",
    "            ('wordext', NumberSelector('TotalWords')),\n",
    "            ('wscaler', StandardScaler()),\n",
    "        ])),\n",
    "    ])),\n",
    "    ('clf', XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.1)),\n",
    "#    ('clf', RandomForestClassifier()),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(X_train, y_train)\n",
    "preds = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, classification_report, confusion_matrix\n",
    "\n",
    "print (\"Accuracy:\", accuracy_score(y_test, preds))\n",
    "print (\"Precision:\", precision_score(y_test, preds))\n",
    "print (classification_report(y_test, preds))\n",
    "print (confusion_matrix(y_test, preds))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOW/Yrtpbb/U7oxo2ECILeT",
   "collapsed_sections": [
    "M2Yb5wSkJuVR"
   ],
   "mount_file_id": "1yUC1b0OPk8gG9FDDuY6K3JpHvZGgIx1d",
   "name": "homeDepot.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}